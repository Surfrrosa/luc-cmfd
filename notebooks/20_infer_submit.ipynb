{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy-Move Forgery Detection - Kaggle Submission\n",
    "\n",
    "This notebook runs inference on the evaluation set and generates `submission.csv`.\n",
    "\n",
    "**Requirements:**\n",
    "- Attach a Kaggle Dataset containing model weights\n",
    "- No internet access (all dependencies pre-installed)\n",
    "- GPU runtime recommended (< 4h target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "from model import CMFDNet\n",
    "from dataset import CMFDDataset, collate_fn\n",
    "from infer import infer_image, create_submission\n",
    "from utils import set_all_seeds, memory_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths (adjust these for Kaggle environment)\n",
    "WEIGHTS_PATH = \"/kaggle/input/luc-cmfd-weights/best_model.pth\"  # From attached dataset\n",
    "IMAGE_DIR = \"/kaggle/input/cmfd-competition/test_images\"  # Test images\n",
    "OUTPUT_PATH = \"submission.csv\"\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'backbone': 'dinov2_vits14',\n",
    "    'freeze_backbone': True,\n",
    "    'patch': 12,\n",
    "    'stride': 4,\n",
    "    'top_k': 5,\n",
    "    'ransac_model': 'similarity',\n",
    "    'inlier_thresh_px': 1.5,\n",
    "    'tta': 'flip',\n",
    "    'post': {\n",
    "        'thr': 0.5,\n",
    "        'min_area': 24,\n",
    "        'morph': {'close': 3, 'open': 0}\n",
    "    }\n",
    "}\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Check AMP support\n",
    "amp_available = torch.cuda.is_available() and torch.cuda.amp.is_autocast_available()\n",
    "print(f\"AMP Available: {amp_available}\")\n",
    "\n",
    "# Set seed\n",
    "set_all_seeds(SEED)\n",
    "print(f\"\\nSeed set to: {SEED}\")\n",
    "\n",
    "# Print config\n",
    "print(f\"\\nConfiguration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating model...\")\n",
    "model = CMFDNet(\n",
    "    backbone=CONFIG['backbone'],\n",
    "    freeze_backbone=CONFIG['freeze_backbone'],\n",
    "    patch=CONFIG['patch'],\n",
    "    stride=CONFIG['stride'],\n",
    "    top_k=CONFIG['top_k']\n",
    ")\n",
    "\n",
    "# Load weights\n",
    "if Path(WEIGHTS_PATH).exists():\n",
    "    print(f\"Loading weights from {WEIGHTS_PATH}...\")\n",
    "    state_dict = torch.load(WEIGHTS_PATH, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    print(\"Weights loaded successfully\")\n",
    "else:\n",
    "    print(f\"WARNING: Weights not found at {WEIGHTS_PATH}\")\n",
    "    print(\"Using randomly initialized weights (for testing only)\")\n",
    "\n",
    "model = model.to(device)\n",
    "model = model.to(memory_format=torch.channels_last)  # Performance optimization\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model ready on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading dataset from {IMAGE_DIR}...\")\n",
    "\n",
    "dataset = CMFDDataset(\n",
    "    image_dir=IMAGE_DIR,\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Dataset loaded: {len(dataset)} images\")\n",
    "print(f\"Batches: {len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting inference...\")\n",
    "print(f\"Target: Complete in < 4 hours\")\n",
    "\n",
    "results = []\n",
    "total_time = 0\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader, desc=\"Inference\"):\n",
    "        images = batch['image'].to(device, memory_format=torch.channels_last)\n",
    "        case_ids = batch['case_id']\n",
    "        original_sizes = batch['original_size']\n",
    "        \n",
    "        batch_start = time.perf_counter()\n",
    "        \n",
    "        # Process each image\n",
    "        for img, case_id, orig_size in zip(images, case_ids, original_sizes):\n",
    "            # Inference\n",
    "            mask = infer_image(\n",
    "                model, img, CONFIG, device,\n",
    "                use_tta=(CONFIG['tta'] != 'none')\n",
    "            )\n",
    "            \n",
    "            # Resize to original if needed\n",
    "            if mask.shape != orig_size:\n",
    "                import cv2\n",
    "                mask = cv2.resize(\n",
    "                    mask.astype(np.uint8),\n",
    "                    (orig_size[1], orig_size[0]),\n",
    "                    interpolation=cv2.INTER_NEAREST\n",
    "                )\n",
    "            \n",
    "            results.append({\n",
    "                'case_id': case_id,\n",
    "                'mask': mask\n",
    "            })\n",
    "        \n",
    "        batch_time = time.perf_counter() - batch_start\n",
    "        total_time += batch_time\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "avg_time_per_image = (total_time / len(results)) * 1000  # ms\n",
    "\n",
    "print(f\"\\nInference complete!\")\n",
    "print(f\"Total time: {elapsed:.2f}s ({elapsed/3600:.2f}h)\")\n",
    "print(f\"Processed: {len(results)} images\")\n",
    "print(f\"Avg time per image: {avg_time_per_image:.2f}ms\")\n",
    "\n",
    "# Memory stats\n",
    "if torch.cuda.is_available():\n",
    "    mem = memory_stats(device)\n",
    "    print(f\"Peak GPU memory: {mem['allocated_gb']:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Creating submission file: {OUTPUT_PATH}\")\n",
    "\n",
    "create_submission(results, OUTPUT_PATH)\n",
    "\n",
    "# Verify submission\n",
    "df = pd.read_csv(OUTPUT_PATH)\n",
    "print(f\"\\nSubmission created successfully!\")\n",
    "print(f\"Total submissions: {len(df)}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "# Stats\n",
    "n_authentic = (df['annotation'] == 'authentic').sum()\n",
    "n_forged = len(df) - n_authentic\n",
    "print(f\"\\nAuthentic: {n_authentic} ({n_authentic/len(df)*100:.1f}%)\")\n",
    "print(f\"Forged: {n_forged} ({n_forged/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Preview\n",
    "print(f\"\\nFirst 10 rows:\")\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Submission Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check required columns\n",
    "assert 'case_id' in df.columns, \"Missing 'case_id' column\"\n",
    "assert 'annotation' in df.columns, \"Missing 'annotation' column\"\n",
    "\n",
    "# Check no missing values\n",
    "assert df.isnull().sum().sum() == 0, \"Submission contains null values\"\n",
    "\n",
    "# Check annotation format\n",
    "for idx, row in df.iterrows():\n",
    "    annot = row['annotation']\n",
    "    if annot != 'authentic':\n",
    "        assert annot.startswith('[') and annot.endswith(']'), \\\n",
    "            f\"Invalid RLE format at row {idx}: {annot}\"\n",
    "\n",
    "print(\"âœ“ Submission format validation passed!\")\n",
    "print(f\"\\nSubmission file ready: {OUTPUT_PATH}\")\n",
    "print(f\"Total runtime: {elapsed:.2f}s ({elapsed/3600:.2f}h)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
