{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LUC-CMFD: Copy-Move Forgery Detection for Kaggle Competition\n",
    "\n",
    "This notebook trains a DINOv2-based forgery detection model and generates submissions.\n",
    "\n",
    "**Competition:** Recod.ai/LUC - Scientific Image Forgery Detection  \n",
    "**Objective:** Detect and segment copy-move forgeries in biomedical images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch torchvision tqdm scikit-image pillow numpy pandas pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Paths (adjust for Kaggle environment)\n",
    "    'data_root': '/kaggle/input/recodai-luc-scientific-image-forgery-detection',\n",
    "    'output_dir': '/kaggle/working/weights',\n",
    "    'submission_path': '/kaggle/working/submission.csv',\n",
    "    \n",
    "    # Training\n",
    "    'seed': 42,\n",
    "    'image_size': 256,\n",
    "    'batch_size': 16,\n",
    "    'val_split': 0.2,\n",
    "    'epochs': 50,\n",
    "    'lr': 1e-4,\n",
    "    'patience': 10,\n",
    "    \n",
    "    # Model\n",
    "    'backbone': 'dinov2_vits14',\n",
    "    'freeze_backbone': True,\n",
    "    'patch': 12,\n",
    "    'stride': 4,\n",
    "    'top_k': 5,\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "Path(CONFIG['output_dir']).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def rle_encode(mask: np.ndarray) -> str:\n",
    "    \"\"\"Encode binary mask to RLE string (1-indexed, row-major order).\"\"\"\n",
    "    if not mask.any():\n",
    "        return \"authentic\"\n",
    "    \n",
    "    # Flatten in row-major order\n",
    "    pixels = mask.flatten()\n",
    "    \n",
    "    # Find transitions\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0]\n",
    "    \n",
    "    # Compute start positions and lengths\n",
    "    starts = runs[::2]\n",
    "    lengths = runs[1::2] - starts\n",
    "    \n",
    "    # Convert to 1-indexed\n",
    "    starts_1idx = starts + 1\n",
    "    \n",
    "    # Interleave\n",
    "    rle_pairs = np.empty(len(starts) * 2, dtype=np.int64)\n",
    "    rle_pairs[::2] = starts_1idx\n",
    "    rle_pairs[1::2] = lengths\n",
    "    \n",
    "    return str(rle_pairs.tolist())\n",
    "\n",
    "set_seed(CONFIG['seed'])\n",
    "print(\"✓ Utilities loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CMFDDataset(Dataset):\n",
    "    \"\"\"Dataset for CMFD competition.\"\"\"\n",
    "    \n",
    "    def __init__(self, root, split='train', image_size=256):\n",
    "        self.root = Path(root)\n",
    "        self.split = split\n",
    "        self.image_size = image_size\n",
    "        self.items = self._build_index()\n",
    "    \n",
    "    def _build_index(self):\n",
    "        items = []\n",
    "        \n",
    "        if self.split == 'train':\n",
    "            # Authentic images\n",
    "            auth_dir = self.root / 'train_images' / 'authentic'\n",
    "            if auth_dir.exists():\n",
    "                for img_path in sorted(auth_dir.glob('*')):\n",
    "                    if img_path.suffix.lower() in ['.png', '.jpg', '.jpeg']:\n",
    "                        items.append({\n",
    "                            'image_path': img_path,\n",
    "                            'mask_path': None,\n",
    "                            'case_id': img_path.stem,\n",
    "                            'is_forged': False\n",
    "                        })\n",
    "            \n",
    "            # Forged images\n",
    "            forg_dir = self.root / 'train_images' / 'forged'\n",
    "            mask_dir = self.root / 'train_masks'\n",
    "            if forg_dir.exists():\n",
    "                for img_path in sorted(forg_dir.glob('*')):\n",
    "                    if img_path.suffix.lower() in ['.png', '.jpg', '.jpeg']:\n",
    "                        case_id = img_path.stem\n",
    "                        mask_path = mask_dir / f\"{case_id}.npy\"\n",
    "                        items.append({\n",
    "                            'image_path': img_path,\n",
    "                            'mask_path': mask_path if mask_path.exists() else None,\n",
    "                            'case_id': case_id,\n",
    "                            'is_forged': True\n",
    "                        })\n",
    "        \n",
    "        elif self.split == 'test':\n",
    "            test_dir = self.root / 'test_images'\n",
    "            for img_path in sorted(test_dir.glob('*')):\n",
    "                if img_path.suffix.lower() in ['.png', '.jpg', '.jpeg']:\n",
    "                    items.append({\n",
    "                        'image_path': img_path,\n",
    "                        'mask_path': None,\n",
    "                        'case_id': img_path.stem,\n",
    "                        'is_forged': None\n",
    "                    })\n",
    "        \n",
    "        return items\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.items[idx]\n",
    "        \n",
    "        # Load image\n",
    "        img = Image.open(item['image_path']).convert('RGB')\n",
    "        orig_size = img.size  # (W, H)\n",
    "        img = img.resize((self.image_size, self.image_size), Image.BILINEAR)\n",
    "        img_array = np.array(img).astype(np.float32) / 255.0\n",
    "        img_tensor = torch.from_numpy(img_array).permute(2, 0, 1)  # (C, H, W)\n",
    "        \n",
    "        # Load mask\n",
    "        if item['mask_path'] and item['mask_path'].exists():\n",
    "            mask = np.load(item['mask_path'])\n",
    "            # Handle multi-channel masks (merge with max)\n",
    "            if mask.ndim == 3:\n",
    "                if mask.shape[0] in [2, 3, 4]:  # (C, H, W)\n",
    "                    mask = mask.max(axis=0)\n",
    "                else:  # (H, W, C)\n",
    "                    mask = mask.max(axis=-1)\n",
    "            mask = (mask > 0).astype(np.uint8)\n",
    "            mask_img = Image.fromarray(mask)\n",
    "            mask_img = mask_img.resize((self.image_size, self.image_size), Image.NEAREST)\n",
    "            mask = np.array(mask_img).astype(np.float32)\n",
    "        else:\n",
    "            mask = np.zeros((self.image_size, self.image_size), dtype=np.float32)\n",
    "        \n",
    "        mask_tensor = torch.from_numpy(mask).unsqueeze(0)  # (1, H, W)\n",
    "        \n",
    "        return {\n",
    "            'image': img_tensor,\n",
    "            'mask': mask_tensor,\n",
    "            'case_id': item['case_id'],\n",
    "            'original_size': orig_size\n",
    "        }\n",
    "\n",
    "print(\"✓ Dataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple correlation module (placeholder - can be enhanced)\n",
    "def self_correlation_simple(feats, top_k=5):\n",
    "    \"\"\"Simplified self-correlation for features.\"\"\"\n",
    "    B, C, H, W = feats.shape\n",
    "    \n",
    "    # Normalize features\n",
    "    feats_norm = F.normalize(feats, dim=1)\n",
    "    \n",
    "    # Reshape to (B, C, N) where N = H*W\n",
    "    feats_flat = feats_norm.view(B, C, -1)\n",
    "    \n",
    "    # Compute correlation matrix (B, N, N)\n",
    "    corr = torch.bmm(feats_flat.transpose(1, 2), feats_flat)\n",
    "    \n",
    "    # Get top-k correlations for each position\n",
    "    topk_vals, _ = torch.topk(corr, k=min(top_k, corr.size(-1)), dim=-1)\n",
    "    \n",
    "    # Reshape back to spatial (B, k, H, W)\n",
    "    corr_map = topk_vals.view(B, -1, top_k).transpose(1, 2)\n",
    "    corr_map = corr_map.view(B, top_k, H, W)\n",
    "    \n",
    "    return corr_map\n",
    "\n",
    "\n",
    "class DinoBackbone(nn.Module):\n",
    "    \"\"\"DINOv2 backbone for feature extraction.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='dinov2_vits14', freeze=True):\n",
    "        super().__init__()\n",
    "        self.freeze = freeze\n",
    "        \n",
    "        try:\n",
    "            # Try loading DINOv2\n",
    "            self.backbone = torch.hub.load('facebookresearch/dinov2', model_name)\n",
    "            self.feat_dim = self.backbone.embed_dim\n",
    "            print(f\"✓ Loaded {model_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load DINOv2, using simple conv backbone\")\n",
    "            # Fallback to simple conv\n",
    "            self.backbone = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, 7, stride=2, padding=3),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(256, 384, 3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(384),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "            self.feat_dim = 384\n",
    "        \n",
    "        if freeze:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.freeze:\n",
    "            self.backbone.eval()\n",
    "            with torch.no_grad():\n",
    "                return self._extract_features(x)\n",
    "        return self._extract_features(x)\n",
    "    \n",
    "    def _extract_features(self, x):\n",
    "        if hasattr(self.backbone, 'get_intermediate_layers'):\n",
    "            # DINOv2 ViT\n",
    "            out = self.backbone.get_intermediate_layers(x, n=1)[0]\n",
    "            B, N, C = out.shape\n",
    "            H = W = int(N ** 0.5)\n",
    "            feats = out.reshape(B, H, W, C).permute(0, 3, 1, 2)\n",
    "        else:\n",
    "            # Simple conv\n",
    "            feats = self.backbone(x)\n",
    "        return feats\n",
    "\n",
    "\n",
    "class CMFDNet(nn.Module):\n",
    "    \"\"\"Complete CMFD network.\"\"\"\n",
    "    \n",
    "    def __init__(self, backbone='dinov2_vits14', freeze_backbone=True, top_k=5):\n",
    "        super().__init__()\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        # Backbone\n",
    "        self.backbone = DinoBackbone(backbone, freeze_backbone)\n",
    "        \n",
    "        # Correlation head\n",
    "        self.corr_head = nn.Sequential(\n",
    "            nn.Conv2d(top_k, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 1, 1)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "            nn.Conv2d(64, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 1, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        feats = self.backbone(x)\n",
    "        \n",
    "        # Self-correlation\n",
    "        corr_map = self_correlation_simple(feats, self.top_k)\n",
    "        \n",
    "        # Process correlation\n",
    "        saliency = self.corr_head(corr_map)\n",
    "        \n",
    "        # Decode\n",
    "        logits = self.decoder(saliency)\n",
    "        \n",
    "        # Upsample to input size\n",
    "        logits = F.interpolate(logits, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        return {'logits': logits}\n",
    "\n",
    "print(\"✓ Model architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(pred, target, smooth=1.0):\n",
    "    \"\"\"Dice loss for segmentation.\"\"\"\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred_flat = pred.view(-1)\n",
    "    target_flat = target.view(-1)\n",
    "    intersection = (pred_flat * target_flat).sum()\n",
    "    dice = (2.0 * intersection + smooth) / (pred_flat.sum() + target_flat.sum() + smooth)\n",
    "    return 1.0 - dice\n",
    "\n",
    "def combined_loss(pred, target):\n",
    "    \"\"\"Combined BCE + Dice loss.\"\"\"\n",
    "    bce = nn.BCEWithLogitsLoss()(pred, target)\n",
    "    dice = dice_loss(pred, target)\n",
    "    return bce + dice\n",
    "\n",
    "def compute_metrics(pred, target):\n",
    "    \"\"\"Compute F1, precision, recall.\"\"\"\n",
    "    pred_binary = (torch.sigmoid(pred) > 0.5).float()\n",
    "    tp = (pred_binary * target).sum()\n",
    "    fp = (pred_binary * (1 - target)).sum()\n",
    "    fn = ((1 - pred_binary) * target).sum()\n",
    "    precision = tp / (tp + fp + 1e-7)\n",
    "    recall = tp / (tp + fn + 1e-7)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-7)\n",
    "    return {'precision': precision.item(), 'recall': recall.item(), 'f1': f1.item()}\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scaler, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_f1 = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=\"Training\")\n",
    "    for batch in pbar:\n",
    "        images = batch['image'].to(device)\n",
    "        masks = batch['mask'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast('cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "            output = model(images)\n",
    "            loss = combined_loss(output['logits'], masks)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            metrics = compute_metrics(output['logits'], masks)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_f1 += metrics['f1']\n",
    "        \n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\", 'f1': f\"{metrics['f1']:.4f}\"})\n",
    "    \n",
    "    return {'loss': total_loss / len(dataloader), 'f1': total_f1 / len(dataloader)}\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, dataloader, device):\n",
    "    \"\"\"Validation loop.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_f1 = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Validation\"):\n",
    "        images = batch['image'].to(device)\n",
    "        masks = batch['mask'].to(device)\n",
    "        \n",
    "        with autocast('cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "            output = model(images)\n",
    "            loss = combined_loss(output['logits'], masks)\n",
    "        \n",
    "        metrics = compute_metrics(output['logits'], masks)\n",
    "        total_loss += loss.item()\n",
    "        total_f1 += metrics['f1']\n",
    "    \n",
    "    return {'loss': total_loss / len(dataloader), 'f1': total_f1 / len(dataloader)}\n",
    "\n",
    "print(\"✓ Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "print(\"Loading dataset...\")\n",
    "full_dataset = CMFDDataset(\n",
    "    root=CONFIG['data_root'],\n",
    "    split='train',\n",
    "    image_size=CONFIG['image_size']\n",
    ")\n",
    "print(f\"Total samples: {len(full_dataset)}\")\n",
    "\n",
    "# Train/val split\n",
    "val_size = int(len(full_dataset) * CONFIG['val_split'])\n",
    "train_size = len(full_dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(\n",
    "    full_dataset, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(CONFIG['seed'])\n",
    ")\n",
    "print(f\"Train: {train_size}, Val: {val_size}\")\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(\"✓ Data loaders created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model\n",
    "model = CMFDNet(\n",
    "    backbone=CONFIG['backbone'],\n",
    "    freeze_backbone=CONFIG['freeze_backbone'],\n",
    "    top_k=CONFIG['top_k']\n",
    ")\n",
    "model = model.to(device)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(memory_format=torch.channels_last)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=CONFIG['lr']\n",
    ")\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=5\n",
    ")\n",
    "\n",
    "# AMP scaler\n",
    "scaler = GradScaler('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"✓ Model setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "best_f1 = 0\n",
    "patience_counter = 0\n",
    "\n",
    "print(f\"Starting training for {CONFIG['epochs']} epochs...\\n\")\n",
    "\n",
    "for epoch in range(CONFIG['epochs']):\n",
    "    print(f\"Epoch {epoch + 1}/{CONFIG['epochs']}\")\n",
    "    \n",
    "    # Train\n",
    "    train_stats = train_epoch(model, train_loader, optimizer, scaler, device)\n",
    "    print(f\"Train - Loss: {train_stats['loss']:.4f}, F1: {train_stats['f1']:.4f}\")\n",
    "    \n",
    "    # Validate\n",
    "    val_stats = validate(model, val_loader, device)\n",
    "    print(f\"Val - Loss: {val_stats['loss']:.4f}, F1: {val_stats['f1']:.4f}\")\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(val_stats['f1'])\n",
    "    \n",
    "    # Save best model\n",
    "    if val_stats['f1'] > best_f1:\n",
    "        best_f1 = val_stats['f1']\n",
    "        patience_counter = 0\n",
    "        save_path = Path(CONFIG['output_dir']) / 'best_model.pth'\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"✓ Saved best model (F1: {best_f1:.4f})\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= CONFIG['patience']:\n",
    "        print(f\"Early stopping after {epoch + 1} epochs\")\n",
    "        break\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(f\"\\n✓ Training complete! Best F1: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "best_model_path = Path(CONFIG['output_dir']) / 'best_model.pth'\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()\n",
    "print(\"✓ Loaded best model\")\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = CMFDDataset(\n",
    "    root=CONFIG['data_root'],\n",
    "    split='test',\n",
    "    image_size=CONFIG['image_size']\n",
    ")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        images = batch['image'].to(device)\n",
    "        case_ids = batch['case_id']\n",
    "        orig_sizes = batch['original_size']\n",
    "        \n",
    "        # Forward pass\n",
    "        with autocast('cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "            output = model(images)\n",
    "        \n",
    "        # Get binary masks\n",
    "        logits = output['logits']\n",
    "        masks = (torch.sigmoid(logits) > 0.5).cpu().numpy()\n",
    "        \n",
    "        # Process each sample\n",
    "        for i, case_id in enumerate(case_ids):\n",
    "            mask = masks[i, 0]  # (H, W)\n",
    "            \n",
    "            # Resize to original size\n",
    "            orig_w, orig_h = orig_sizes[i]\n",
    "            mask_img = Image.fromarray((mask * 255).astype(np.uint8))\n",
    "            mask_img = mask_img.resize((orig_w, orig_h), Image.NEAREST)\n",
    "            mask_resized = (np.array(mask_img) > 127).astype(np.uint8)\n",
    "            \n",
    "            # Encode to RLE\n",
    "            rle = rle_encode(mask_resized)\n",
    "            \n",
    "            predictions.append({\n",
    "                'id': case_id,\n",
    "                'mask_rle': rle\n",
    "            })\n",
    "\n",
    "print(f\"✓ Generated {len(predictions)} predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame(predictions)\n",
    "submission_df = submission_df.sort_values('id')\n",
    "\n",
    "# Save submission\n",
    "submission_df.to_csv(CONFIG['submission_path'], index=False)\n",
    "\n",
    "print(f\"✓ Submission saved to {CONFIG['submission_path']}\")\n",
    "print(f\"\\nSubmission preview:\")\n",
    "print(submission_df.head(10))\n",
    "print(f\"\\nTotal submissions: {len(submission_df)}\")\n",
    "print(f\"Authentic images: {(submission_df['mask_rle'] == 'authentic').sum()}\")\n",
    "print(f\"Forged images: {(submission_df['mask_rle'] != 'authentic').sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "Download the `submission.csv` file and submit to Kaggle competition."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
